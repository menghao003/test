"""
ä¼˜åŒ–åçš„æµ‹è¯•è„šæœ¬
åŒ…å«æ”¹è¿›çš„ç”Ÿæˆç­–ç•¥ã€åå¤„ç†ç­›é€‰å’Œç»¼åˆè¯„ä¼°
"""

import os
import sys
import argparse
import json
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.diffusion_model import ConditionalDiffusionModel
from models.structure_generator import StructureGenerator, COMMON_2D_ELEMENTS
from models.optimization import PropertyPredictor, compute_pareto_front
from dataset.material_dataset import MaterialDataset, NUM_ATOM_TYPES
from utils.geo_utils import MaterialEvaluator, HERActivityCalculator
from utils.vis import (
    plot_her_performance, 
    plot_stability_curve, 
    plot_generated_structures,
    plot_comparison_table
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OptimizedMaterialGenerator:
    """
    ä¼˜åŒ–çš„ææ–™ç”Ÿæˆå™¨
    
    æ”¹è¿›ï¼š
    - å¤šé˜¶æ®µç”Ÿæˆç­–ç•¥
    - åå¤„ç†ç­›é€‰
    - Paretoå‰æ²¿åˆ†æ
    - è‡ªé€‚åº”é‡‡æ ·
    """
    
    def __init__(self,
                 model_path: str = None,
                 device: str = 'cpu'):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ConditionalDiffusionModel(
            num_atom_types=NUM_ATOM_TYPES + 1,
            hidden_dim=64,
            time_dim=64,
            num_blocks=2,
            num_timesteps=100,
            condition_dim=3
        ).to(device)
        
        # åŠ è½½æƒé‡
        if model_path and os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"âœ“ åŠ è½½æ¨¡å‹: {model_path}")
        else:
            logger.warning("âš  ä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹")
        
        self.model.eval()
        
        # ç»“æ„ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°ï¼‰
        self.structure_generator = StructureGenerator(
            self.model,
            num_atom_types=NUM_ATOM_TYPES + 1
        )
        
        # ææ–™è¯„ä¼°å™¨
        self.evaluator = MaterialEvaluator()
        self.her_calculator = HERActivityCalculator()
    
    def generate_with_filtering(self,
                                 num_materials: int = 20,
                                 target_delta_g: float = 0.0,
                                 target_stability: float = 0.8,
                                 target_synthesizability: float = 0.8,
                                 num_atoms_range: tuple = (4, 12),
                                 filter_threshold: float = 0.5,
                                 diversity_weight: float = 0.3) -> List[Dict]:
        """
        ç”Ÿæˆå¹¶ç­›é€‰ææ–™ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        æ”¹è¿›ç­–ç•¥ï¼š
        1. ç”Ÿæˆ2-3å€æ•°é‡çš„å€™é€‰ææ–™
        2. åå¤„ç†ç­›é€‰ä½è´¨é‡ææ–™
        3. ä¿æŒå¤šæ ·æ€§
        4. Paretoå‰æ²¿åˆ†æ
        
        Args:
            num_materials: ç›®æ ‡ç”Ÿæˆæ•°é‡
            target_delta_g: ç›®æ ‡Î”G_H
            target_stability: ç›®æ ‡ç¨³å®šæ€§
            target_synthesizability: ç›®æ ‡å¯åˆæˆæ€§
            num_atoms_range: åŸå­æ•°èŒƒå›´
            filter_threshold: ç­›é€‰é˜ˆå€¼
            diversity_weight: å¤šæ ·æ€§æƒé‡
        
        Returns:
            ç­›é€‰åçš„ææ–™ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹ä¼˜åŒ–ç”Ÿæˆæµç¨‹")
        logger.info("=" * 70)
        logger.info(f"ç›®æ ‡æ•°é‡: {num_materials}")
        logger.info(f"ç›®æ ‡: Î”G_H={target_delta_g:.3f}, ç¨³å®šæ€§={target_stability:.2f}, å¯åˆæˆæ€§={target_synthesizability:.2f}")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆå€™é€‰ææ–™ï¼ˆç”Ÿæˆ2-3å€æ•°é‡ï¼‰
        num_candidates = num_materials * 3
        logger.info(f"\nğŸ“ é˜¶æ®µ1: ç”Ÿæˆ {num_candidates} ä¸ªå€™é€‰ææ–™...")
        
        structures = self.structure_generator.generate_structures(
            num_structures=num_candidates,
            num_atoms_range=num_atoms_range,
            target_delta_g=target_delta_g,
            target_stability=target_stability,
            target_synthesizability=target_synthesizability,
            device=self.device,
            temperature=1.0,
            guidance_scale=1.8,  # å¢å¼ºå¼•å¯¼
            max_attempts=3
        )
        
        logger.info(f"âœ“ æˆåŠŸç”Ÿæˆ {len(structures)} ä¸ªç»“æ„")
        
        if not structures:
            logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆç»“æ„")
            return []
        
        # ç¬¬äºŒé˜¶æ®µï¼šè¯„ä¼°æ‰€æœ‰å€™é€‰ææ–™
        logger.info(f"\nğŸ“Š é˜¶æ®µ2: è¯„ä¼°å€™é€‰ææ–™...")
        
        candidates = []
        for i, structure in enumerate(structures):
            try:
                eval_result = self.evaluator.evaluate(structure)
                eval_result['structure'] = structure
                eval_result['index'] = i
                candidates.append(eval_result)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"  å·²è¯„ä¼° {i+1}/{len(structures)} ä¸ªææ–™")
                    
            except Exception as e:
                logger.debug(f"è¯„ä¼°ææ–™ {i+1} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"âœ“ æˆåŠŸè¯„ä¼° {len(candidates)} ä¸ªææ–™")
        
        if not candidates:
            logger.error("æœªèƒ½è¯„ä¼°ä»»ä½•ææ–™")
            return []
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåå¤„ç†ç­›é€‰
        logger.info(f"\nğŸ” é˜¶æ®µ3: åå¤„ç†ç­›é€‰ (é˜ˆå€¼={filter_threshold})...")
        
        filtered_candidates = self._filter_materials(
            candidates,
            filter_threshold=filter_threshold,
            target_delta_g=target_delta_g
        )
        
        logger.info(f"âœ“ ç­›é€‰åå‰©ä½™ {len(filtered_candidates)} ä¸ªé«˜è´¨é‡ææ–™")
        
        # ç¬¬å››é˜¶æ®µï¼šParetoå‰æ²¿åˆ†æ
        logger.info(f"\nğŸ¯ é˜¶æ®µ4: Paretoå‰æ²¿åˆ†æ...")
        
        pareto_indices = compute_pareto_front(filtered_candidates)
        pareto_materials = [filtered_candidates[i] for i in pareto_indices]
        
        logger.info(f"âœ“ è¯†åˆ«å‡º {len(pareto_materials)} ä¸ªParetoæœ€ä¼˜ææ–™")
        
        # ç¬¬äº”é˜¶æ®µï¼šå¤šæ ·æ€§é€‰æ‹©
        logger.info(f"\nğŸŒˆ é˜¶æ®µ5: å¤šæ ·æ€§é€‰æ‹©...")
        
        final_materials = self._select_diverse_materials(
            filtered_candidates,
            pareto_materials,
            num_materials,
            diversity_weight
        )
        
        logger.info(f"âœ“ æœ€ç»ˆé€‰æ‹© {len(final_materials)} ä¸ªææ–™")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(final_materials, pareto_materials)
        
        return final_materials
    
    def _filter_materials(self,
                         materials: List[Dict],
                         filter_threshold: float,
                         target_delta_g: float) -> List[Dict]:
        """
        åå¤„ç†ç­›é€‰ææ–™
        
        ç­›é€‰æ ‡å‡†ï¼š
        - ç»¼åˆè¯„åˆ† > threshold
        - HERæ´»æ€§åœ¨åˆç†èŒƒå›´å†…
        - æ— é‡å¤åŒ–å­¦å¼
        """
        filtered = []
        seen_formulas = set()
        
        for mat in materials:
            # æ£€æŸ¥ç»¼åˆè¯„åˆ†
            if mat['overall_score'] < filter_threshold:
                continue
            
            # æ£€æŸ¥HERæ´»æ€§ï¼ˆä¸è¦å¤ªåç¦»ç›®æ ‡ï¼‰
            if abs(mat['delta_g'] - target_delta_g) > 0.25:
                continue
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            formula = mat['formula']
            if formula in seen_formulas:
                continue
            
            filtered.append(mat)
            seen_formulas.add(formula)
        
        return filtered
    
    def _select_diverse_materials(self,
                                  all_materials: List[Dict],
                                  pareto_materials: List[Dict],
                                  num_select: int,
                                  diversity_weight: float) -> List[Dict]:
        """
        é€‰æ‹©å¤šæ ·åŒ–çš„ææ–™
        
        ç­–ç•¥ï¼š
        - ä¼˜å…ˆé€‰æ‹©Paretoæœ€ä¼˜ææ–™
        - æ ¹æ®åŒ–å­¦å¼å¤šæ ·æ€§é€‰æ‹©å…¶ä»–ææ–™
        - å¹³è¡¡æ€§èƒ½å’Œå¤šæ ·æ€§
        """
        selected = []
        selected_formulas = set()
        
        # 1. é¦–å…ˆé€‰æ‹©Paretoæœ€ä¼˜ææ–™
        pareto_sorted = sorted(pareto_materials, 
                              key=lambda x: x['overall_score'], 
                              reverse=True)
        
        for mat in pareto_sorted:
            if len(selected) >= num_select:
                break
            if mat['formula'] not in selected_formulas:
                selected.append(mat)
                selected_formulas.add(mat['formula'])
        
        # 2. å¦‚æœè¿˜éœ€è¦æ›´å¤šææ–™ï¼ŒæŒ‰ç»¼åˆè¯„åˆ†é€‰æ‹©
        if len(selected) < num_select:
            remaining = [m for m in all_materials if m['formula'] not in selected_formulas]
            remaining_sorted = sorted(remaining,
                                    key=lambda x: x['overall_score'],
                                    reverse=True)
            
            for mat in remaining_sorted:
                if len(selected) >= num_select:
                    break
                # æ£€æŸ¥å…ƒç´ å¤šæ ·æ€§
                elements = set(mat['formula'].split())
                is_diverse = True
                for sel_mat in selected[-3:]:  # åªä¸æœ€è¿‘çš„3ä¸ªæ¯”è¾ƒ
                    sel_elements = set(sel_mat['formula'].split())
                    overlap = len(elements & sel_elements) / len(elements | sel_elements)
                    if overlap > 0.7:  # ç›¸ä¼¼åº¦è¿‡é«˜
                        is_diverse = False
                        break
                
                if is_diverse:
                    selected.append(mat)
                    selected_formulas.add(mat['formula'])
        
        return selected
    
    def _print_statistics(self, final_materials: List[Dict], pareto_materials: List[Dict]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“ˆ ç”Ÿæˆç»“æœç»Ÿè®¡")
        logger.info("=" * 70)
        
        # åŸºæœ¬ç»Ÿè®¡
        delta_g_values = [m['delta_g'] for m in final_materials]
        stability_values = [m['stability_score'] for m in final_materials]
        synth_values = [m['synthesizability'] for m in final_materials]
        overall_scores = [m['overall_score'] for m in final_materials]
        
        logger.info(f"å¹³å‡ Î”G_H: {np.mean(np.abs(delta_g_values)):.4f} Â± {np.std(delta_g_values):.4f} eV")
        logger.info(f"å¹³å‡ç¨³å®šæ€§: {np.mean(stability_values):.4f} Â± {np.std(stability_values):.4f}")
        logger.info(f"å¹³å‡å¯åˆæˆæ€§: {np.mean(synth_values):.4f} Â± {np.std(synth_values):.4f}")
        logger.info(f"å¹³å‡ç»¼åˆè¯„åˆ†: {np.mean(overall_scores):.4f} Â± {np.std(overall_scores):.4f}")
        
        # è´¨é‡ç»Ÿè®¡
        excellent_count = sum(1 for m in final_materials if m.get('is_excellent', False))
        promising_count = sum(1 for m in final_materials if m['is_promising'])
        
        logger.info(f"\nâœ¨ ä¼˜ç§€ææ–™ (Î”G_H<0.08): {excellent_count} ({excellent_count/len(final_materials)*100:.1f}%)")
        logger.info(f"ğŸŒŸ æœ‰å‰æ™¯ææ–™ (Î”G_H<0.12): {promising_count} ({promising_count/len(final_materials)*100:.1f}%)")
        logger.info(f"ğŸ¯ Paretoæœ€ä¼˜ææ–™: {len(pareto_materials)}")
        
        # Top-5ææ–™
        top5 = sorted(final_materials, key=lambda x: x['overall_score'], reverse=True)[:5]
        logger.info(f"\nğŸ† Top-5 ææ–™:")
        for i, mat in enumerate(top5, 1):
            logger.info(
                f"  {i}. {mat['formula']:<25} | "
                f"Î”G_H={mat['delta_g']:>7.3f} | "
                f"ç¨³å®šæ€§={mat['stability_score']:.3f} | "
                f"å¯åˆæˆ={mat['synthesizability']:.3f} | "
                f"ç»¼åˆ={mat['overall_score']:.3f}"
            )
    
    def save_structures(self, results: List[Dict], output_dir: str = 'generated'):
        """ä¿å­˜ç”Ÿæˆçš„ç»“æ„"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        for result in results:
            structure = result['structure']
            formula = result['formula'].replace(' ', '')
            filename = f"{formula}_{result['index']:03d}.cif"
            filepath = output_path / filename
            
            structure.to(filename=str(filepath))
            saved_files.append(str(filepath))
            
        logger.info(f"\nğŸ’¾ ä¿å­˜äº† {len(saved_files)} ä¸ªç»“æ„æ–‡ä»¶åˆ° {output_dir}")
        
        return saved_files


def run_optimized_test(args):
    """è¿è¡Œä¼˜åŒ–çš„æµ‹è¯•æµç¨‹"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸš€ ä¼˜åŒ–æµ‹è¯•æµç¨‹")
    logger.info("=" * 70)
    logger.info(f"æ¨¡å‹: {args.model_path}")
    logger.info(f"è®¾å¤‡: {args.device}")
    logger.info(f"éšæœºç§å­: {args.seed}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    results_dir = Path(args.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = OptimizedMaterialGenerator(
        model_path=args.model_path,
        device=args.device
    )
    
    # ç”Ÿæˆå¹¶ç­›é€‰ææ–™
    results = generator.generate_with_filtering(
        num_materials=args.num_samples,
        target_delta_g=args.target_delta_g,
        target_stability=args.target_stability,
        target_synthesizability=args.target_synth,
        filter_threshold=args.filter_threshold,
        diversity_weight=args.diversity_weight
    )
    
    if not results:
        logger.error("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆææ–™")
        return
    
    # ä¿å­˜ç”Ÿæˆçš„ç»“æ„
    generator.save_structures(results, str(results_dir / 'generated_optimized'))
    
    # æå–æ•°æ®ç”¨äºå¯è§†åŒ–
    delta_g_values = [r['delta_g'] for r in results]
    stability_scores = [r['stability_score'] for r in results]
    formation_energies = [r['formation_energy'] for r in results]
    synthesizability = [r['synthesizability'] for r in results]
    
    # ç”Ÿæˆå¯è§†åŒ–
    logger.info(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # HERæ€§èƒ½å›¾
    plot_her_performance(
        delta_g_values,
        save_path=str(results_dir / 'her_performance_optimized.png'),
        title='Optimized Materials HER Activity'
    )
    
    # ç¨³å®šæ€§æ›²çº¿
    plot_stability_curve(
        formation_energies,
        stability_scores,
        synthesizability,
        save_path=str(results_dir / 'stability_curve_optimized.png')
    )
    
    # ç»“æ„æ‘˜è¦ï¼ˆå–å‰9ä¸ªï¼‰
    plot_generated_structures(
        results[:min(9, len(results))],
        save_path=str(results_dir / 'generated_structures_optimized.png')
    )
    
    # ä¸baselineå¯¹æ¯”
    baseline_results = {
        'avg_delta_g': 0.25,
        'stability': 0.65,
        'synthesis_rate': 0.45
    }
    
    # ä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼è®¡ç®—åˆæˆç‡
    synth_threshold = 0.55
    our_results = {
        'avg_delta_g': np.mean(np.abs(delta_g_values)),
        'stability': np.mean(stability_scores),
        'synthesis_rate': sum(1 for s in synthesizability if s > synth_threshold) / len(synthesizability)
    }
    
    plot_comparison_table(
        baseline_results,
        our_results,
        save_path=str(results_dir / 'comparison_table_optimized.png')
    )
    
    # è®¡ç®—æ”¹è¿›ç‡
    improvement = {
        'delta_g': (baseline_results['avg_delta_g'] - our_results['avg_delta_g']) / baseline_results['avg_delta_g'] * 100,
        'stability': (our_results['stability'] - baseline_results['stability']) / baseline_results['stability'] * 100,
        'synthesis_rate': (our_results['synthesis_rate'] - baseline_results['synthesis_rate']) / baseline_results['synthesis_rate'] * 100
    }
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    stats = {
        'num_generated': len(results),
        'avg_delta_g': float(np.mean(delta_g_values)),
        'std_delta_g': float(np.std(delta_g_values)),
        'avg_abs_delta_g': float(np.mean(np.abs(delta_g_values))),
        'avg_stability': float(np.mean(stability_scores)),
        'std_stability': float(np.std(stability_scores)),
        'avg_synthesizability': float(np.mean(synthesizability)),
        'std_synthesizability': float(np.std(synthesizability)),
        'excellent_count': sum(1 for r in results if r.get('is_excellent', False)),
        'promising_count': sum(1 for r in results if r['is_promising']),
        'improvement_vs_baseline': improvement,
        'top_materials': [
            {
                'formula': r['formula'],
                'delta_g': float(r['delta_g']),
                'stability': float(r['stability_score']),
                'synthesizability': float(r['synthesizability']),
                'overall_score': float(r['overall_score']),
                'is_excellent': r.get('is_excellent', False),
                'is_promising': r['is_promising']
            }
            for r in sorted(results, key=lambda x: x['overall_score'], reverse=True)[:20]
        ]
    }
    
    with open(results_dir / 'test_results_optimized.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“‹ æœ€ç»ˆæ‘˜è¦")
    logger.info("=" * 70)
    logger.info(f"ç”Ÿæˆææ–™æ•°: {stats['num_generated']}")
    logger.info(f"å¹³å‡ |Î”G_H|: {stats['avg_abs_delta_g']:.4f} Â± {stats['std_delta_g']:.4f} eV")
    logger.info(f"å¹³å‡ç¨³å®šæ€§: {stats['avg_stability']:.4f} Â± {stats['std_stability']:.4f}")
    logger.info(f"å¹³å‡å¯åˆæˆæ€§: {stats['avg_synthesizability']:.4f} Â± {stats['std_synthesizability']:.4f}")
    logger.info(f"ä¼˜ç§€ææ–™æ•°: {stats['excellent_count']} ({stats['excellent_count']/stats['num_generated']*100:.1f}%)")
    logger.info(f"æœ‰å‰æ™¯ææ–™æ•°: {stats['promising_count']} ({stats['promising_count']/stats['num_generated']*100:.1f}%)")
    
    logger.info("\nğŸ“ˆ ç›¸æ¯”Baselineæ”¹è¿›:")
    logger.info(f"  Î”G_H: {improvement['delta_g']:+.1f}%")
    logger.info(f"  ç¨³å®šæ€§: {improvement['stability']:+.1f}%")
    logger.info(f"  åˆæˆç‡: {improvement['synthesis_rate']:+.1f}%")
    
    logger.info(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {results_dir}")
    logger.info("=" * 70)


def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–æµ‹è¯•äºŒç»´ææ–™ç”Ÿæˆæ¨¡å‹')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_path', type=str, default='checkpoints/best_model.pt',
                        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--num_samples', type=int, default=10,
                        help='ç›®æ ‡ç”Ÿæˆæ ·æœ¬æ•°')
    parser.add_argument('--target_delta_g', type=float, default=0.0,
                        help='ç›®æ ‡Î”G_Hå€¼')
    parser.add_argument('--target_stability', type=float, default=0.85,
                        help='ç›®æ ‡ç¨³å®šæ€§ï¼ˆæé«˜ç›®æ ‡ï¼‰')
    parser.add_argument('--target_synth', type=float, default=0.85,
                        help='ç›®æ ‡å¯åˆæˆæ€§ï¼ˆæé«˜ç›®æ ‡ï¼‰')
    
    # ç­›é€‰å‚æ•°
    parser.add_argument('--filter_threshold', type=float, default=0.55,
                        help='åå¤„ç†ç­›é€‰é˜ˆå€¼')
    parser.add_argument('--diversity_weight', type=float, default=0.3,
                        help='å¤šæ ·æ€§æƒé‡')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--results_dir', type=str, default='results',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # è¿è¡Œä¼˜åŒ–æµ‹è¯•
    run_optimized_test(args)


if __name__ == "__main__":
    main()

